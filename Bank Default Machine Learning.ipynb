{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('UCI_Credit_Card.csv', index_col =0, na_values=\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EDUCATION'] = df['EDUCATION'].replace([0,5,6],4)\n",
    "print(df['EDUCATION'].value_counts())\n",
    "\n",
    "fil = (df['MARRIAGE'] == 0)\n",
    "df.loc[fil, 'MARRIAGE'] = 3\n",
    "print(df['MARRIAGE'].value_counts())\n",
    "\n",
    "df = df.rename({'PAY_0':'PAY_1'}, axis ='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage(df, columns=5):\n",
    "    print('Memory usage ----')\n",
    "    memory_per_column = df.memory_usage(deep=True) / 1024 ** 2\n",
    "    print(f'Top {columns} columns by memory (MB):')\n",
    "    print(memory_per_column.sort_values(ascending=False) \\\n",
    "    .head(columns))\n",
    "    print(f'Total size: {memory_per_column.sum():.4f} MB')\n",
    "    \n",
    "memory_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().round().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(df.loc[df.SEX==1, 'AGE'].dropna(),\n",
    "    hist=False, color='blue',\n",
    "    kde_kws={\"shade\": True},\n",
    "    ax=ax, label='Male')\n",
    "sns.distplot(df.loc[df.SEX==2, 'AGE'].dropna(),\n",
    "    hist=False, color='red',\n",
    "    kde_kws={\"shade\": True},\n",
    "    ax=ax, label='Female')\n",
    "ax.set_title('Distribution of age')\n",
    "ax.legend(title='Gender:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot('default.payment.next.month', hue='SEX',data=df, orient='h')\n",
    "ax.set_title('Target variable distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x='EDUCATION', y='LIMIT_BAL',hue='SEX', split=True, data=df)\n",
    "ax.set_title('Limit balance per education level distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby(\"EDUCATION\")['default.payment.next.month'] \\\n",
    "    .value_counts(normalize=True) \\\n",
    "    .unstack() \\\n",
    "    .plot(kind='barh', stacked='True')\n",
    "ax.set_title('Percentage of default per education level')\n",
    "ax.legend(title='Default', bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(25,8)})\n",
    "sns.set_context(\"talk\", font_scale=0.7)\n",
    "sns.heatmap(df.corr(), cmap='Oranges', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_plot = sns.pairplot(df[['EDUCATION', 'LIMIT_BAL','default.payment.next.month']])\n",
    "pair_plot.fig.suptitle('Pairplot', y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#df.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dum = pd.get_dummies(df, columns=[\"EDUCATION\"], prefix=[\"Edu\"] )\n",
    "df_dum = df.merge(df_dum, how='outer')\n",
    "\n",
    "df_dum2 = pd.get_dummies(df, columns=[\"SEX\"], prefix=[\"SEX\"] )\n",
    "df_dum2 = df_dum.merge(df_dum2, how='outer')\n",
    "\n",
    "df_dum3 = pd.get_dummies(df, columns=[\"MARRIAGE\"], prefix=[\"MARRIAGE\"] )\n",
    "df_dum3 = df_dum2.merge(df_dum3, how='outer')\n",
    "\n",
    "df_dum4 = pd.get_dummies(df, columns=[\"PAY_1\"], prefix=[\"p1\"] )\n",
    "df_dum4 = df_dum3.merge(df_dum4, how='outer')\n",
    "\n",
    "df_dum5 = pd.get_dummies(df, columns=[\"PAY_2\"], prefix=[\"p2\"] )\n",
    "df_dum5 = df_dum4.merge(df_dum5, how='outer')\n",
    "\n",
    "df_dum6 = pd.get_dummies(df, columns=[\"PAY_3\"], prefix=[\"p3\"] )\n",
    "df_dum6 = df_dum5.merge(df_dum6, how='outer')\n",
    "\n",
    "df_dum7 = pd.get_dummies(df, columns=[\"PAY_4\"], prefix=[\"p4\"] )\n",
    "df_dum7 = df_dum6.merge(df_dum7, how='outer')\n",
    "\n",
    "df_dum8 = pd.get_dummies(df, columns=[\"PAY_5\"], prefix=[\"p5\"] )\n",
    "df_dum8 = df_dum7.merge(df_dum8, how='outer')\n",
    "\n",
    "df_dum9 = pd.get_dummies(df, columns=[\"PAY_6\"], prefix=[\"p6\"] )\n",
    "df_dum9 = df_dum8.merge(df_dum9, how='outer')\n",
    "\n",
    "df_dum9 = df_dum9.drop(['SEX','EDUCATION','MARRIAGE','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'],axis=1)\n",
    "print(df_dum9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scale = prep.MinMaxScaler().fit(df_dum9)\n",
    "credit_minmax = minmax_scale.transform(df_dum9)\n",
    "credit_minmax = pd.DataFrame(credit_minmax, columns = list(df_dum9))\n",
    "credit_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = credit_minmax.drop([\"default.payment.next.month\"],axis=1)\n",
    "y = credit_minmax[\"default.payment.next.month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missingno.matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = 'gini', random_state = 100, max_depth = 3, min_samples_leaf = 5)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "\n",
    "prediction = clf_gini.predict(X_test)\n",
    "print(\"Decision Tree Model Report\")\n",
    "report = classification_report(y_test, prediction)\n",
    "print(report)\n",
    "\n",
    "#Plot the Matrix\n",
    "confusion_matrix = cm(y_test, prediction)\n",
    "print(confusion_matrix)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, prediction)\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,prediction,normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = clf_gini.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train, y_train],axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([X_test, y_test],axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0, count_class_1 = df_train['default.payment.next.month'].value_counts()\n",
    "\n",
    "df_majority = df_train[df_train['default.payment.next.month']==0]\n",
    "df_minority = df_train[df_train['default.payment.next.month']==1]\n",
    "\n",
    "df_minority_upsampled = df_minority.sample(count_class_0, replace=True)\n",
    "df_upsampled = pd.concat([df_majority,df_minority_upsampled],axis=0)\n",
    " \n",
    "print('Random Oversampling:')\n",
    "print(df_upsampled['default.payment.next.month'].value_counts())\n",
    " \n",
    "\n",
    "df_upsampled['default.payment.next.month'].value_counts().plot(kind='bar', title='Count (default.payment.next.month)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled = df_upsampled.drop([\"default.payment.next.month\"],axis=1)\n",
    "y_train_upsampled = df_upsampled[\"default.payment.next.month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = 'gini', random_state = 100, max_depth = 3, min_samples_leaf = 5)\n",
    "clf_gini.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "prediction = clf_gini.predict(X_test)\n",
    "print(\"Decision Tree Model Report\")\n",
    "report = classification_report(y_test, prediction)\n",
    "print(report)\n",
    "\n",
    "#Plot the Matrix\n",
    "confusion_matrix = cm(y_test, prediction)\n",
    "print(confusion_matrix)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, prediction)\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,prediction,normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_prob = clf_gini.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=1000,\n",
    "                            random_state=9,\n",
    "                            n_estimators=11,\n",
    "                            verbose=False)\n",
    "\n",
    "clf.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "prediction = clf.predict(X_test)\n",
    "print(\"Decision Tree Model Report\")\n",
    "report = classification_report(y_test, prediction)\n",
    "print(report)\n",
    "\n",
    "#Plot the Matrix\n",
    "confusion_matrix = cm(y_test, prediction)\n",
    "print(confusion_matrix)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, prediction)\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,prediction,normalize=True)\n",
    "plt.show()\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "lr_probs = clf.predict_proba(X_test)\n",
    "\n",
    "lr_probs = lr_probs[:, 1]\n",
    "\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_upsampled,y_train_upsampled)\n",
    "pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\")\n",
    "response = accuracy_score(y_test,pred)\n",
    "print(response)\n",
    "\n",
    "confusion_matrix = cm(y_test, pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, pred)\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,pred,normalize=True)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "y_pred_prob = knn.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "lr_probs = knn.predict_proba(X_test)\n",
    "\n",
    "lr_probs = lr_probs[:, 1]\n",
    "\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_upsampled, y_train_upsampled)\n",
    "prediction = logreg.predict(X_test)\n",
    "print(\"Accuracy:\")\n",
    "response = accuracy_score(y_test,prediction)\n",
    "print(response)\n",
    "\n",
    "prediction = dict()\n",
    "prediction['Logistic'] = logreg.predict(X_test)\n",
    "print('f1 Score:' ,metrics.f1_score(y_test, prediction['Logistic']))\n",
    "\n",
    "confusion_matrix = cm(y_test, prediction['Logistic'])\n",
    "print(confusion_matrix)\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, prediction['Logistic'])\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,prediction['Logistic'],normalize=True)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, prediction['Logistic']))\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "lr_probs = logreg.predict_proba(X_test)\n",
    "\n",
    "lr_probs = lr_probs[:, 1]\n",
    "\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "outcome = []\n",
    "model_names = []\n",
    "models = [  \n",
    "          ('DecTree', DecisionTreeClassifier()),\n",
    "          ('RandomForest', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('LogReg', LogisticRegression()),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "k_fold_validation = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "for model_name, model in models:\n",
    "    k_fold_validation = model_selection.StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "    results = model_selection.cross_val_score(model, X_train_upsampled, y_train_upsampled, cv=k_fold_validation, scoring='accuracy')\n",
    "    outcome.append(results)\n",
    "    model_names.append(model_name)\n",
    "    output_message = \"%s| Mean=%f STD=%f\" % (model_name, results.mean(), results.std())\n",
    "    print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "for model_name, model in models:\n",
    "    print('\\n',model,'Parameters currently in use:\\n')\n",
    "    pprint(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {\"n_estimators\":[5,10,50,100,250],\n",
    "               \"max_depth\":[2,4,8,16,32,None],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 150, cv = 9, verbose=2, random_state=42, n_jobs = -1)\n",
    "rfc_random.fit(X_train_upsampled, y_train_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=1000,\n",
    "                            random_state=9,\n",
    "                            n_estimators=11,\n",
    "                            verbose=False)\n",
    "\n",
    "clf.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "prediction = clf.predict(X_test)\n",
    "print(\"Decision Tree Model Report\")\n",
    "report = classification_report(y_test, prediction)\n",
    "print(report)\n",
    "\n",
    "#Plot the Matrix\n",
    "confusion_matrix = cm(y_test, prediction)\n",
    "print(confusion_matrix)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, prediction)\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,prediction,normalize=True)\n",
    "plt.show()\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clft = RandomForestClassifier(n_estimators = 100,\n",
    "                             min_samples_split = 2,\n",
    "                             min_samples_leaf = 1,\n",
    "                             max_features = 'sqrt',\n",
    "                             max_depth = None)\n",
    "\n",
    "clft.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "prediction = clft.predict(X_test)\n",
    "print(\"Decision Tree Model Report\")\n",
    "report = classification_report(y_test, prediction)\n",
    "print(report)\n",
    "\n",
    "#Plot the Matrix\n",
    "confusion_matrix = cm(y_test, prediction)\n",
    "print(confusion_matrix)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, prediction)\n",
    "plt.show()\n",
    "skplt.metrics.plot_confusion_matrix(y_test,prediction,normalize=True)\n",
    "plt.show()\n",
    "\n",
    "y_pred_prob = clft.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "print(metrics.auc(recall, precision))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision,\n",
    "label=f'PR-AUC = {metrics.auc(recall, precision):.2f}')\n",
    "ax.set(title='Precision-Recall Curve',\n",
    "xlabel='Recall',\n",
    "ylabel='Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "lr_probs = clft.predict_proba(X_test)\n",
    "\n",
    "lr_probs = lr_probs[:, 1]\n",
    "\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_upsampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feat_imp = pd.DataFrame(clft.feature_importances_,\n",
    "                            index=feat_names,\n",
    "                            columns=['mdi'])\n",
    "rf_feat_imp = rf_feat_imp.sort_values('mdi', ascending=False)\n",
    "rf_feat_imp['cumul_importance_mdi'] = np.cumsum(rf_feat_imp.mdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clft.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "plt.styple.use('ggplot')\n",
    "sorted_idx = clft.feature_importances_.argsort()\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.barh(feature_names[sorted_idx], clft.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clft)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
